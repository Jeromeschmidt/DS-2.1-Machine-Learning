{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_text= ['Send us your password', 'review us', 'Send your password', 'Send us your account']\n",
    "ham_text= ['Send us your review', 'review your password']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'send': 3, 'us': 3, 'your': 3, 'password': 2, 'review': 1, 'account': 1}\n",
      "{'send': 1, 'us': 1, 'your': 2, 'review': 2, 'password': 1}\n"
     ]
    }
   ],
   "source": [
    "spam = dict()\n",
    "ham = dict()\n",
    "for phrase in spam_text:\n",
    "    words = phrase.split()\n",
    "    for word in words:\n",
    "        if word.lower() in spam:\n",
    "            spam[word.lower()] += 1\n",
    "        else:\n",
    "            spam[word.lower()] = 1\n",
    "            \n",
    "for phrase in ham_text:\n",
    "    words = phrase.split()\n",
    "    for word in words:\n",
    "        if word.lower() in ham:\n",
    "            ham[word.lower()] += 1\n",
    "        else:\n",
    "            ham[word.lower()] = 1\n",
    "            \n",
    "print(spam)\n",
    "print(ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "send: 0.23076923076923078\n",
      "us: 0.23076923076923078\n",
      "your: 0.23076923076923078\n",
      "password: 0.15384615384615385\n",
      "review: 0.07692307692307693\n",
      "account: 0.07692307692307693\n"
     ]
    }
   ],
   "source": [
    "### Prob for each word in spam email\n",
    "for word in spam.keys():\n",
    "    print(word + \": \"+ str(spam[word]/sum(spam.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "send: 0.14285714285714285\n",
      "us: 0.14285714285714285\n",
      "your: 0.2857142857142857\n",
      "review: 0.2857142857142857\n",
      "password: 0.14285714285714285\n"
     ]
    }
   ],
   "source": [
    "### Prob for each word in ham email\n",
    "for word in ham.keys():\n",
    "    print(word + \": \"+ str(ham[word]/sum(ham.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6829268292682927"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### prob of spam if word is password, use bayes rule\n",
    "(0.15384615384615385*(4/6))/((0.15384615384615385*(4/6)) + (0.14285714285714285*(2/6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3170731707317073"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### prob of ham if word is password, use bayes rule\n",
    "(0.14285714285714285*(2/6))/((0.14285714285714285*(2/6)) + (0.15384615384615385*(4/6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. which line(s) of the method calculates the probabilty of ham and spam?\n",
    "    lines: 8-11\n",
    "        self.num_messages['spam'] = sum(1 for label in Y if label == 1)\n",
    "        self.num_messages['ham'] = sum(1 for label in Y if label == 0)\n",
    "        self.log_class_priors['spam'] = math.log(self.num_messages['spam'] / n)\n",
    "        self.log_class_priors['ham'] = math.log(self.num_messages['ham'] / n)\n",
    "\n",
    "4. which line(s) of the method calculates the spam and ham dictionaries?\n",
    "    lines: 15-24\n",
    "        for x, y in zip(X, Y):\n",
    "            c = 'spam' if y == 1 else 'ham'\n",
    "            counts = self.get_word_counts(self.tokenize(x))\n",
    "            for word, count in counts.items():\n",
    "                if word not in self.vocab:\n",
    "                    self.vocab.add(word)\n",
    "                if word not in self.word_counts[c]:\n",
    "                    self.word_counts[c][word] = 0.0\n",
    "\n",
    "                self.word_counts[c][word] += count\n",
    "            \n",
    "5. which line(s) compares the scores of ham or spam based on log probabilities?\n",
    "    lines 20-23\n",
    "        if spam_score > ham_score:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                               text\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "5572\n",
      "{'spam': 544, 'ham': 3635}\n",
      "0.75\n",
      "[[1190    0]\n",
      " [  72  131]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class SpamDetector(object):\n",
    "    \"\"\"Implementation of Naive Bayes for binary classification\"\"\"\n",
    "\n",
    "    # clean up our string by removing punctuation\n",
    "    def clean(self, s):\n",
    "        translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "        return s.translate(translator)\n",
    "\n",
    "    #  tokenize our string into words\n",
    "    def tokenize(self, text):\n",
    "        text = self.clean(text).lower()\n",
    "        return re.split(\"\\W+\", text)\n",
    "\n",
    "    # count up how many of each word appears in a list of words.\n",
    "    def get_word_counts(self, words):\n",
    "        word_counts = {}\n",
    "        for word in words:\n",
    "            word_counts[word] = word_counts.get(word, 0.0) + 1.0\n",
    "        return word_counts\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"Fit our classifier\n",
    "        Arguments:\n",
    "            X {list} -- list of document contents\n",
    "            y {list} -- correct labels\n",
    "        \"\"\"\n",
    "        self.num_messages = {}\n",
    "        self.log_class_priors = {}\n",
    "        self.word_counts = {}\n",
    "        self.vocab = set()\n",
    "\n",
    "        # Compute log class priors (the probability that any given message is spam/ham),\n",
    "        # by counting how many messages are spam/ham, \n",
    "        # dividing by the total number of messages, and taking the log.\n",
    "        n = len(X)\n",
    "        self.num_messages['spam'] = sum(1 for label in Y if label == 'spam')\n",
    "        self.num_messages['ham'] = sum(1 for label in Y if label == 'ham')\n",
    "        self.log_class_priors['spam'] = math.log(self.num_messages['spam'] / n )\n",
    "        self.log_class_priors['ham'] = math.log(self.num_messages['ham'] / n )\n",
    "        self.word_counts['spam'] = {}\n",
    "        self.word_counts['ham'] = {}\n",
    "\n",
    "        # for each (document, label) pair, tokenize the document into words.\n",
    "        for x, y in zip(X, Y):\n",
    "            c = 'spam' if y == 'spam' else 'ham'\n",
    "            counts = self.get_word_counts(self.tokenize(x))\n",
    "            # For each word, either add it to the vocabulary for spam/ham, \n",
    "            # if it isnâ€™t already there, and update the number of counts. \n",
    "            for word, count in counts.items():\n",
    "                # Add that word to the global vocabulary.\n",
    "                if word not in self.vocab:\n",
    "                    self.vocab.add(word)\n",
    "                if word not in self.word_counts[c]:\n",
    "                    self.word_counts[c][word] = 0.0\n",
    "\n",
    "                self.word_counts[c][word] += count\n",
    "\n",
    "    # function to actually output the class label for new data.\n",
    "    def predict(self, X):\n",
    "        result = []\n",
    "        # Given a document...\n",
    "        for x in X:\n",
    "            counts = self.get_word_counts(self.tokenize(x))\n",
    "            spam_score = 0\n",
    "            ham_score = 0\n",
    "            # We iterate through each of the words...\n",
    "            for word, _ in counts.items():\n",
    "                if word not in self.vocab: continue\n",
    "                # ... and compute log p(w_i|Spam), and sum them all up. The same will happen for Ham\n",
    "                # add Laplace smoothing\n",
    "                # https://medium.com/syncedreview/applying-multinomial-naive-bayes-to-nlp-problems-a-practical-explanation-4f5271768ebf\n",
    "                log_w_given_spam = math.log( (self.word_counts['spam'].get(word, 0.0) + 1) / (self.num_messages['spam'] + len(self.vocab)) )\n",
    "                log_w_given_ham = math.log( (self.word_counts['ham'].get(word, 0.0) + 1) / (self.num_messages['ham'] + len(self.vocab)) )\n",
    "\n",
    "                spam_score += log_w_given_spam\n",
    "                ham_score += log_w_given_ham\n",
    "            \n",
    "            # Then we add the log class priors...\n",
    "            spam_score += self.log_class_priors['spam']\n",
    "            ham_score += self.log_class_priors['ham']\n",
    "\n",
    "            # ... and check to see which score is bigger for that document.\n",
    "            # Whichever is larger, that is the predicted label!\n",
    "            if spam_score > ham_score:\n",
    "                result.append('spam')\n",
    "            else:\n",
    "                result.append('ham')\n",
    "        return result\n",
    "        \n",
    "\n",
    "# TODO: Fill in the below function to make a prediction, \n",
    "# your answer should match the final number in the below output (0.9641)\n",
    "if __name__ == '__main__':\n",
    "    data = pd.read_csv('Datasets/spam.csv',encoding='latin-1')\n",
    "    data = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "    data = data.rename(columns={\"v1\":'label', \"v2\":'text'})\n",
    "    print(data.head())\n",
    "    tags = data[\"label\"]\n",
    "    texts = data[\"text\"]\n",
    "    X, y = texts, tags\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "    \n",
    "    SD = SpamDetector()\n",
    "    SD.fit(X_train,y_train)\n",
    "    \n",
    "    y_pred = SD.predict(X_test.values)\n",
    "    \n",
    "    print(len(X))\n",
    "    \n",
    "    print(SD.num_messages)\n",
    "    \n",
    "    print(sum(SD.num_messages.values())/len(X))\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9626704953338119"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1224+117)/(1224+117+51+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                               text\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "  (0, 3286)\t1\n",
      "  (0, 4747)\t2\n",
      "  (0, 1896)\t1\n",
      "  (0, 875)\t2\n",
      "  (0, 6599)\t2\n",
      "  (0, 801)\t1\n",
      "  (0, 5258)\t1\n",
      "  (0, 7209)\t3\n",
      "  (0, 1559)\t1\n",
      "  (0, 913)\t1\n",
      "  (0, 6623)\t3\n",
      "  (0, 1050)\t1\n",
      "  (0, 5980)\t1\n",
      "  (0, 3530)\t1\n",
      "  (0, 919)\t1\n",
      "  (0, 802)\t1\n",
      "  (0, 819)\t1\n",
      "  (0, 5712)\t1\n",
      "  (0, 6727)\t1\n",
      "  (0, 2112)\t1\n",
      "  (0, 5065)\t2\n",
      "  (0, 7373)\t1\n",
      "  (0, 4176)\t2\n",
      "  (0, 1535)\t2\n",
      "  (0, 6604)\t1\n",
      "  :\t:\n",
      "  (4176, 4747)\t1\n",
      "  (4176, 3252)\t1\n",
      "  (4176, 3416)\t1\n",
      "  (4176, 2304)\t1\n",
      "  (4176, 6638)\t1\n",
      "  (4176, 4450)\t1\n",
      "  (4176, 7163)\t1\n",
      "  (4176, 4219)\t1\n",
      "  (4176, 1590)\t1\n",
      "  (4176, 3439)\t1\n",
      "  (4176, 4833)\t1\n",
      "  (4176, 4894)\t1\n",
      "  (4177, 3647)\t1\n",
      "  (4177, 3252)\t1\n",
      "  (4177, 6074)\t1\n",
      "  (4177, 4125)\t1\n",
      "  (4177, 3162)\t1\n",
      "  (4177, 4766)\t1\n",
      "  (4177, 1848)\t1\n",
      "  (4177, 5232)\t1\n",
      "  (4177, 6953)\t1\n",
      "  (4178, 4274)\t1\n",
      "  (4178, 7295)\t1\n",
      "  (4178, 6055)\t1\n",
      "  (4178, 1695)\t1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9856424982053122"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "# Prepare the dataset\n",
    "data = pd.read_csv('Datasets/spam.csv',encoding='latin-1')\n",
    "data = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "data = data.rename(columns={\"v1\":'label', \"v2\":'text'})\n",
    "print(data.head())\n",
    "tags = data[\"label\"]\n",
    "texts = data[\"text\"]\n",
    "\n",
    "# create texts and tags\n",
    "X, y = texts, tags\n",
    "\n",
    "# split the data into train vs test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# transform text into numerical vectors\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_dtm = vectorizer.fit_transform(X_train)\n",
    "print(X_train_dtm)\n",
    "\n",
    "# instantiate Multinomial Naive Bayes model\n",
    "nb = MultinomialNB()\n",
    "# fit to model, with the trained part of the dataset\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "X_test_dtm = vectorizer.transform(X_test)\n",
    "# make prediction\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "# test accurarcy of prediction\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
